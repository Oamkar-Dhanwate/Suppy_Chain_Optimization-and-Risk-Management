# ğŸ“¦ Supply Chain Optimization & Risk Management Dashboard

A comprehensive **data-driven dashboard** built with **Python** and **Streamlit** to analyze supply chain performance, predict late delivery risks, and provide actionable insights for logistics management.

---

## ğŸš€ Live Demo  
[Add your Streamlit Community Cloud deployment link here]

---

## âœ¨ Key Features
- **Executive KPIs**: Quick insights with OTIF (On-Time-In-Full) Rate & Perfect Order Rate.  
- **Interactive Analytics**: Dynamic charts & geographical heatmaps to spot high-risk regions.  
- **ML-Powered Risk Prediction**: Real-time late delivery risk detection with Random Forest.  
- **Optimal Shipping Recommendation**: Suggests best shipping mode balancing risk & profit.  
- **Dynamic Filtering & Export**: Filter by region, mode, date & export results as CSV.  

---

## ğŸ“¸ Dashboard Preview  
(Add a screenshot of your running dashboard here)

---

## ğŸ“‚ Project Structure
supply_chain_optimization/
â”œâ”€â”€ ğŸ“‚ dashboard/ # Streamlit dashboard
â”‚ â””â”€â”€ app.py # Main Streamlit application
â”œâ”€â”€ ğŸ“‚ data/ # Data storage
â”‚ â””â”€â”€ raw/ # Raw dataset
â”œâ”€â”€ ğŸ“‚ notebooks/ # Jupyter notebooks (EDA, modeling, experiments)
â”œâ”€â”€ ğŸ“‚ src/ # Source code for the data pipeline
â”‚ â”œâ”€â”€ ğŸ“¦ data/ # Data loading & preprocessing
â”‚ â”œâ”€â”€ ğŸ“¦ features/ # Feature engineering scripts
â”‚ â””â”€â”€ ğŸ“¦ models/ # ML models (training & prediction)
â”œâ”€â”€ run_pipeline.py # Master script to process data & train models
â””â”€â”€ requirements.txt # Project dependencies

---

## ğŸ›  Tech Stack
- **Backend**: Python, Pandas, Scikit-learn  
- **Machine Learning**: Random Forest (classification), K-Means (segmentation), Prophet (forecasting)  
- **Dashboard**: Streamlit  
- **Visualization**: Plotly Express  

---

## âš™ï¸ How to Run Locally
```bash
# Clone the repository
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name

# Create & activate virtual environment
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run data pipeline (process data + train ML models)
python run_pipeline.py

# Launch dashboard
streamlit run dashboard/app.py
